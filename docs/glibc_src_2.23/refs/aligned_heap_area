<def f='glibc_src_2.23/malloc/arena.c' l='396' type='char *'/>
<use f='glibc_src_2.23/malloc/arena.c' l='425' u='r' c='new_heap'/>
<use f='glibc_src_2.23/malloc/arena.c' l='427' u='r' c='new_heap'/>
<use f='glibc_src_2.23/malloc/arena.c' l='429' u='w' c='new_heap'/>
<use f='glibc_src_2.23/malloc/arena.c' l='447' u='w' c='new_heap'/>
<use f='glibc_src_2.23/malloc/arena.c' l='540' u='r' c='heap_trim'/>
<use f='glibc_src_2.23/malloc/arena.c' l='541' u='w' c='heap_trim'/>
<doc f='glibc_src_2.23/malloc/arena.c' l='388'>/* If consecutive mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...) calls return decreasing
   addresses as opposed to increasing, new_heap would badly fragment the
   address space.  In that case remember the second HEAP_MAX_SIZE part
   aligned to HEAP_MAX_SIZE from last mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...)
   call (if it is already aligned) and try to reuse it next time.  We need
   no locking for it, as kernel ensures the atomicity for us - worst case
   we&apos;ll call mmap (addr, HEAP_MAX_SIZE, ...) for some value of addr in
   multiple threads, but only one will succeed.  */</doc>
