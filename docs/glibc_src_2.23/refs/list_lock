<def f='glibc_src_2.23/malloc/arena.c' l='92' type='mutex_t'/>
<use f='glibc_src_2.23/malloc/arena.c' l='151' u='a' c='__malloc_fork_lock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='151' u='a' c='__malloc_fork_lock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='151' u='a' c='__malloc_fork_lock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='151' u='a' c='__malloc_fork_lock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='176' u='a' c='__malloc_fork_unlock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='176' u='a' c='__malloc_fork_unlock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='176' u='a' c='__malloc_fork_unlock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='176' u='a' c='__malloc_fork_unlock_parent'/>
<use f='glibc_src_2.23/malloc/arena.c' l='207' u='a' c='__malloc_fork_unlock_child'/>
<use f='glibc_src_2.23/malloc/arena.c' l='677' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='677' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='677' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='677' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='687' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='687' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='687' u='a' c='_int_new_arena'/>
<use f='glibc_src_2.23/malloc/arena.c' l='687' u='a' c='_int_new_arena'/>
<doc f='glibc_src_2.23/malloc/arena.c' l='80'>/* list_lock prevents concurrent writes to the next member of struct
   malloc_state objects.

   Read access to the next member is supposed to synchronize with the
   atomic_write_barrier and the write to the next member in
   _int_new_arena.  This suffers from data races; see the FIXME
   comments in _int_new_arena and reused_arena.

   list_lock also prevents concurrent forks.  At the time list_lock is
   acquired, no arena lock must have been acquired, but it is
   permitted to acquire arena locks subsequently, while list_lock is
   acquired.  */</doc>
