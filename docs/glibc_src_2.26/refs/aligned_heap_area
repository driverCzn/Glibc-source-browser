<def f='glibc_src_2.26/malloc/arena.c' l='455' type='char *'/>
<use f='glibc_src_2.26/malloc/arena.c' l='484' u='r' c='new_heap'/>
<use f='glibc_src_2.26/malloc/arena.c' l='486' u='r' c='new_heap'/>
<use f='glibc_src_2.26/malloc/arena.c' l='488' u='w' c='new_heap'/>
<use f='glibc_src_2.26/malloc/arena.c' l='506' u='w' c='new_heap'/>
<use f='glibc_src_2.26/malloc/arena.c' l='599' u='r' c='heap_trim'/>
<use f='glibc_src_2.26/malloc/arena.c' l='600' u='w' c='heap_trim'/>
<doc f='glibc_src_2.26/malloc/arena.c' l='447'>/* If consecutive mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...) calls return decreasing
   addresses as opposed to increasing, new_heap would badly fragment the
   address space.  In that case remember the second HEAP_MAX_SIZE part
   aligned to HEAP_MAX_SIZE from last mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...)
   call (if it is already aligned) and try to reuse it next time.  We need
   no locking for it, as kernel ensures the atomicity for us - worst case
   we&apos;ll call mmap (addr, HEAP_MAX_SIZE, ...) for some value of addr in
   multiple threads, but only one will succeed.  */</doc>
