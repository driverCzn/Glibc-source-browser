<def f='glibc_src_2.25/malloc/arena.c' l='449' type='char *'/>
<use f='glibc_src_2.25/malloc/arena.c' l='478' u='r' c='new_heap'/>
<use f='glibc_src_2.25/malloc/arena.c' l='480' u='r' c='new_heap'/>
<use f='glibc_src_2.25/malloc/arena.c' l='482' u='w' c='new_heap'/>
<use f='glibc_src_2.25/malloc/arena.c' l='500' u='w' c='new_heap'/>
<use f='glibc_src_2.25/malloc/arena.c' l='593' u='r' c='heap_trim'/>
<use f='glibc_src_2.25/malloc/arena.c' l='594' u='w' c='heap_trim'/>
<doc f='glibc_src_2.25/malloc/arena.c' l='441'>/* If consecutive mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...) calls return decreasing
   addresses as opposed to increasing, new_heap would badly fragment the
   address space.  In that case remember the second HEAP_MAX_SIZE part
   aligned to HEAP_MAX_SIZE from last mmap (0, HEAP_MAX_SIZE &lt;&lt; 1, ...)
   call (if it is already aligned) and try to reuse it next time.  We need
   no locking for it, as kernel ensures the atomicity for us - worst case
   we&apos;ll call mmap (addr, HEAP_MAX_SIZE, ...) for some value of addr in
   multiple threads, but only one will succeed.  */</doc>
